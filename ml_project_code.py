# -*- coding: utf-8 -*-
"""ML_Project_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IaKvri-Rg8JvcivlvnnDzGje_8-0IpAy

Group Members : Arihant Singh (2019298) , Smiti Chhabra (2019112) , Tushar Mahajan (2019280) ; Group Number : 20
"""

import requests
import pandas as pd
from google.colab import drive
from google.colab import files
import pickle
from imblearn.over_sampling import SMOTE 
from scipy import stats
import numpy as np
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.cluster import AgglomerativeClustering
from xgboost import XGBClassifier
import xgboost
from sklearn import svm
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import preprocessing
from scipy import stats
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error

drive.mount('/content/gdrive')

"""Importing necessary libraries :

Loading the dataset :
"""

url = "framingham.csv"
df = pd.read_csv(url)

"""Data overview"""

#Printing the dataset
print(df)
#male -> 1, female -> 0 ; subject has the disease -> 1, subject does not have the disease -> 0

#Printing the number of rows and columns in the dataset
print(df.shape)

#Printing the summary of the dataset
print(df.info())

#Printing some statistical data values for the dataset
print(df.describe())

#Printing count of all unique values in the dataset
print(df.value_counts)

#Printing all the columns in the dataset
print(df.columns)

#Printing sum of all null values in the dataset
print(df.isnull().sum())

#Printing the count of entries in each feature column
print(df.count())

#Printing the sum of all duplicates in the dataset
print(df.duplicated().sum())

#Printing the maximum of values for all the feature columns
print(df.max())

#Printing the minimum of values for all the feature columns
print(df.min())

#Printing the data types for all the feature columns
print(df.dtypes)

"""Data Visualization

Pie graphs for all unique values of a feature column
"""

#For 'male' feature column
labels = []
sizes = []

for x, y in df['male'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'age' feature column
labels = []
sizes = []

for x, y in df['age'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'education' feature column
labels = []
sizes = []

for x, y in df['education'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'currentSmoker' feature column
labels = []
sizes = []

for x, y in df['currentSmoker'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'cigsPerDay' feature column
labels = []
sizes = []

for x, y in df['cigsPerDay'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'BPMeds' feature column
labels = []
sizes = []

for x, y in df['BPMeds'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'prevalentStroke' feature column
labels = []
sizes = []

for x, y in df['prevalentStroke'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'prevalentHyp' feature column
labels = []
sizes = []

for x, y in df['prevalentHyp'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'diabetes' feature column
labels = []
sizes = []

for x, y in df['diabetes'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'totChol' feature column
labels = []
sizes = []

for x, y in df['totChol'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'sysBP' feature column
labels = []
sizes = []

for x, y in df['sysBP'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'diaBP' feature column
labels = []
sizes = []

for x, y in df['diaBP'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'BMI' feature column
labels = []
sizes = []

for x, y in df['BMI'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'heartRate' feature column
labels = []
sizes = []

for x, y in df['heartRate'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'glucose' feature column
labels = []
sizes = []

for x, y in df['glucose'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

#For 'TenYearCHD' feature column
labels = []
sizes = []

for x, y in df['TenYearCHD'].value_counts().items():
    labels.append(x)
    sizes.append(y)

plt.pie(sizes, labels=labels)

plt.show()

"""Histograms for all feature columns"""

#For 'male' feature column
print(plt.hist(df['male']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'age' feature column
print(plt.hist(df['age']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'education' feature column
print(plt.hist(df['education']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'currentSmoker' feature column
print(plt.hist(df['currentSmoker']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'cigsPerDay' feature column
print(plt.hist(df['cigsPerDay']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'BPMeds' feature column
print(plt.hist(df['BPMeds']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'prevalentStroke' feature column
print(plt.hist(df['prevalentStroke']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'prevalentHyp' feature column
print(plt.hist(df['prevalentHyp']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'diabetes' feature column
print(plt.hist(df['diabetes']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'totChol' feature column
print(plt.hist(df['totChol']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'sysBP' feature column
print(plt.hist(df['sysBP']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'diaBP' feature column
print(plt.hist(df['diaBP']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'BMI' feature column
print(plt.hist(df['BMI']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'heartRate' feature column
print(plt.hist(df['heartRate']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'glucose' feature column
print(plt.hist(df['glucose']))
plt.xlabel("Values")
plt.ylabel("Frequency")

#For 'TenYearCHD' feature column
print(plt.hist(df['TenYearCHD']))
plt.xlabel("Values")
plt.ylabel("Frequency")

"""Plots for each feature column"""

#For 'male' feature column
print(plt.plot(df['male'].value_counts()))

#For 'age' feature column
print(plt.plot(df['age']))

#For 'education' feature column
print(plt.plot(df['education']))

#For 'currentSmoker' feature column
print(plt.plot(df['currentSmoker'].value_counts()))

#For 'cigsPerDay' feature column
print(plt.plot(df['cigsPerDay']))

#For 'BPMeds' feature column
print(plt.plot(df['BPMeds']))

#For 'prevalentStroke' feature column
print(plt.plot(df['prevalentStroke'].value_counts()))

#For 'prevalentHyp' feature column
print(plt.plot(df['prevalentHyp'].value_counts()))

#For 'diabetes' feature column
print(plt.plot(df['diabetes'].value_counts()))

#For 'totChol' feature column
print(plt.plot(df['totChol']))

#For 'sysBP' feature column
print(plt.plot(df['sysBP']))

#For 'diaBP' feature column
print(plt.plot(df['diaBP']))

#For 'BMI' feature column
print(plt.plot(df['BMI']))

#For 'heartRate' feature column
print(plt.plot(df['heartRate']))

#For 'glucose' feature column
print(plt.plot(df['glucose']))

#For 'TenYearCHD' feature column
print(plt.plot(df['TenYearCHD'].value_counts()))

"""Scatter Plots for each column with the output column ("TenYearCHD")"""

#For 'male' feature column
print(plt.scatter(df['male'],df['TenYearCHD']))

#For 'age' feature column
print(plt.scatter(df['age'],df['TenYearCHD']))

#For 'education' feature column
print(plt.scatter(df['education'],df['TenYearCHD']))

#For 'currentSmoker' feature column
print(plt.scatter(df['currentSmoker'],df['TenYearCHD']))

#For 'cigsPerDay' feature column
print(plt.scatter(df['cigsPerDay'],df['TenYearCHD']))

#For 'BPMeds' feature column
print(plt.scatter(df['BPMeds'],df['TenYearCHD']))

#For 'prevalentStroke' feature column
print(plt.scatter(df['prevalentStroke'],df['TenYearCHD']))

#For 'prevalentHyp' feature column
print(plt.scatter(df['prevalentHyp'],df['TenYearCHD']))

#For 'diabetes' feature column
print(plt.scatter(df['diabetes'],df['TenYearCHD']))

#For 'totChol' feature column
print(plt.scatter(df['totChol'],df['TenYearCHD']))

#For 'sysBP' feature column
print(plt.scatter(df['sysBP'],df['TenYearCHD']))

#For 'diaBP' feature column
print(plt.scatter(df['diaBP'],df['TenYearCHD']))

#For 'BMI' feature column
print(plt.scatter(df['BMI'],df['TenYearCHD']))

#For 'heartRate' feature column
print(plt.scatter(df['heartRate'],df['TenYearCHD']))

#For 'glucose' feature column
print(plt.scatter(df['glucose'],df['TenYearCHD']))

"""KDE Plots for each feature column"""

#For "male" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['male'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Gender', fontsize=12)
plt.legend()
plt.show()

#For "age" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['age'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Age', fontsize=12)
plt.legend()
plt.show()

#For "education" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['education'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Education', fontsize=12)
plt.legend()
plt.show()

#For "currentSmoker" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['currentSmoker'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Smoking Status', fontsize=12)
plt.legend()
plt.show()

#For "cigsPerDay" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['cigsPerDay'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Cigarettes Per Day', fontsize=12)
plt.legend()
plt.show()

#For "BPMeds" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['BPMeds'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of BP Medication', fontsize=12)
plt.legend()
plt.show()

#For "prevalentStroke" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['prevalentStroke'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Prevalent Stroke', fontsize=12)
plt.legend()
plt.show()

#For "prevalentHyp" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['prevalentHyp'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Prevalent HYP', fontsize=12)
plt.legend()
plt.show()

#For "diabetes" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['diabetes'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Diabetes', fontsize=12)
plt.legend()
plt.show()

#For "totChol" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['totChol'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Total Cholestrol', fontsize=12)
plt.legend()
plt.show()

#For "sysBP" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['sysBP'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of SysBP', fontsize=12)
plt.legend()
plt.show()

#For "diaBP" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['diaBP'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of DiaBP', fontsize=12)
plt.legend()
plt.show()

#For "BMI" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['BMI'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Body Mass Index', fontsize=12)
plt.legend()
plt.show()

#For "heartRate" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['heartRate'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Heart Rate', fontsize=12)
plt.legend()
plt.show()

#For "glucose" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['glucose'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Glucose', fontsize=12)
plt.legend()
plt.show()

#For "TenYearCHD" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(df['TenYearCHD'], shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Ten Year CHD', fontsize=12)
plt.legend()
plt.show()

"""Heatmap for the correlation matrix of the dataset"""

plt.figure(figsize=(16,10), dpi= 80)
df.corr()
dataplot = sns.heatmap(df.corr(), cmap="YlGnBu", annot=True)

"""Converting integer type feature columns to float type feature columns"""

df['male'] = df['male'].astype(float)
df['age'] = df['age'].astype(float)
df['currentSmoker'] = df['currentSmoker'].astype(float)
df['prevalentStroke'] = df['prevalentStroke'].astype(float)
df['prevalentHyp'] = df['prevalentHyp'].astype(float)
df['diabetes'] = df['diabetes'].astype(float)
df['TenYearCHD'] = df['TenYearCHD'].astype(float)

"""Pre-Processing Steps

*Note : We tried different combinations of preprocessing steps to reach the final conclusion along with evaluating the different models at each of the stages using the metrics mentioned below(also mentioned in the report and ppt)*

Filling missing values for a feature column with their respective means
"""

#For "cisPerDay" feature column
meanCigs = df['cigsPerDay'].mean()
df['cigsPerDay'] = df['cigsPerDay'].fillna(meanCigs)

#For "totChol" feature column
meanTotChol = df['totChol'].mean()
df['totChol'] = df['totChol'].fillna(meanTotChol)

#For "BPMeds" feature column
meanBPMeds = df['BPMeds'].mean()
df['BPMeds'] = df['BPMeds'].fillna(meanBPMeds)
df = df.dropna()

"""Dropping all null values"""

df = df.dropna()

"""Dropping feature columns using feature selection"""

#Dropping "education" feature column
df = df.drop(['education'], axis = 1)
#Dropping "glucose" feature column
df = df.drop(['glucose'], axis = 1)
#Dropping "diaBP" feature column
df = df.drop(['diaBP'], axis = 1)
#Dropping "currentSmoker" feature column
df = df.drop(['currentSmoker'], axis = 1)

"""Grouping of features with continuous values

Note : We tried grouping of features for various continuous valued features but didn't include it in the final preprocessing steps
"""

#For "age" feature column
def get_age_bin(age_column):
  age_column_copy=np.empty(age_column.shape,dtype=object)
  age_column_copy[age_column<40]=1
  age_column_copy[np.where((age_column<50) & (age_column>=40))]=2
  age_column_copy[np.where((age_column<60) & (age_column>=50))]=3
  age_column_copy[np.where((age_column<=70) & (age_column>=60))]=4
  return age_column_copy

def bin_age(df):
  df['age_categorical']=get_age_bin(np.array(df['age']))
  df = df.drop(['age'], axis=1)
  #df = pd.get_dummies(df)
  return df

df = bin_age(df)

#For "totChol" feature column
def get_totChol_bin(totChol_column):
  totChol_column_copy=np.empty(totChol_column.shape,dtype=object)
  totChol_column_copy[totChol_column<200]=1
  totChol_column_copy[np.where((totChol_column<240) & (totChol_column>=200))]=2
  totChol_column_copy[totChol_column>=240]=3
  return totChol_column_copy

def bin_totChol(df):
  df['totChol_categorical']=get_totChol_bin(np.array(df['totChol']))
  df = df.drop(['totChol'], axis=1)
  #df = pd.get_dummies(df)
  return df

df = bin_totChol(df)

"""Removing ouliers using z-score method"""

z_score_data = np.abs(stats.zscore(df))
print(z_score_data)
threshold = 3
temp_arr_new = np.where(z_score_data > threshold)[0]
df.drop(df.index[temp_arr_new],inplace=True)
print(df)

"""Balancing the dataset using SMOTE"""

df3 = df['TenYearCHD']
df2 = df.drop(['TenYearCHD'],axis = 1)
sm = SMOTE()
X_res, y_res = sm.fit_resample(df2, df3)
print(X_res.shape)
print(y_res.shape)

"""Scaling

*Note : We tried normalization but didn't include it in the final preprocessing steps*
"""

#Normalization using "MinMaxScaler" technique
sc = MinMaxScaler()
scaled_data = sc.fit_transform(df)

#Standardization 
Standardisation = preprocessing.StandardScaler()
x_after_Standardisation = Standardisation.fit_transform(df)

"""Dividing the dataset using train : test ratio as 8 : 2"""

x_train, x_test, y_train, y_test = train_test_split(df2, df3, test_size = 0.2,random_state = 0)

"""Dimensional reduction using PCA"""

pca = PCA(n_components = 2)
 
X_train = pca.fit_transform(x_train)
X_test = pca.transform(x_test)

"""Making different models and evaluating them based on different evaluation metrics ( Accuracy, Precision, Recall, and Confusion Matrix)"""

#Making "Logistic Regression" model
lr = LogisticRegression()
#Fitting the model
lr.fit(x_train, y_train)
#Predicting the output
y_pred1 = lr.predict(x_test)
#Printing the accuracy
print(metrics.accuracy_score(y_test, y_pred1))
#Printing the precision
print(metrics.precision_score(y_test,y_pred1))
#Printing the recall
print(metrics.recall_score(y_test,y_pred1))
#Printing the Confusion Matrix
print("Confusion Matrix(in format [TN,FN],[FP,TP]):",confusion_matrix(y_test,y_pred1))

lr_probs=lr.predict_proba(x_test)
lr_probs=lr_probs[:,1]
lr_auc=roc_auc_score(y_test,lr_probs)
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)

path1 = F"/content/gdrive/My Drive/Colab Notebooks/lr_model"
lr_weights = pickle.dump(lr,open(path1,'wb'))

#Making "Decision Tree Classifier" model
dtc = DecisionTreeClassifier()
#Fitting the model
dtc.fit(x_train, y_train)
#Predicting the output
y_pred4 = dtc.predict(x_test)
#Printing the accuracy
print(metrics.accuracy_score(y_test, y_pred4))
#Printing the precision
print(metrics.precision_score(y_test,y_pred4))
#Printing the recall
print(metrics.recall_score(y_test,y_pred4))
#Printing the Confusion Matrix
print("Confusion Matrix(in format [TN,FN],[FP,TP]):",confusion_matrix(y_test,y_pred4))

dtc_probs=dtc.predict_proba(x_test)
dtc_probs=dtc_probs[:,1]
dtc_auc=roc_auc_score(y_test,dtc_probs)
dtc_fpr, dtc_tpr, _ = roc_curve(y_test, dtc_probs)

path1 = F"/content/gdrive/My Drive/Colab Notebooks/dtc_model"
dtc_weights = pickle.dump(dtc,open(path1,'wb'))

#Making "GaussianNB" model
gnb = GaussianNB()
#Fitting the model
gnb.fit(x_train, y_train)
#Predicting the output
y_pred5 = gnb.predict(x_test)
#Printing the accuracy
print(metrics.accuracy_score(y_test, y_pred5))
#printing the precision
print(metrics.precision_score(y_test,y_pred5))
#Printing the recall
print(metrics.recall_score(y_test,y_pred5))
#Printing the Confusion Matrix
print("Confusion Matrix(in format [TN,FN],[FP,TP]):",confusion_matrix(y_test,y_pred5))

gnb_probs=gnb.predict_proba(x_test)
gnb_probs=gnb_probs[:,1]
gnb_auc=roc_auc_score(y_test,gnb_probs)
gnb_fpr, gnb_tpr, _ = roc_curve(y_test, gnb_probs)

path1 = F"/content/gdrive/My Drive/Colab Notebooks/gnb_model"
gnb_weights = pickle.dump(gnb,open(path1,'wb'))

#Making the "Random Forest" model
rfc = RandomForestClassifier()
#Fitting the model
rfc.fit(x_train, y_train)
#Predicting the output
y_pred3 = rfc.predict(x_test)
#Printing the accuracy
print(metrics.accuracy_score(y_test, y_pred3))
#Printing the precision
print(metrics.precision_score(y_test,y_pred3))
#Printing the recall
print(metrics.recall_score(y_test,y_pred3))
#Printing the Confusion Matrix
print("Confusion Matrix(in format [TN,FN],[FP,TP]):",confusion_matrix(y_test,y_pred3))

rfc_probs=rfc.predict_proba(x_test)
rfc_probs=rfc_probs[:,1]
rfc_auc=roc_auc_score(y_test,rfc_probs)
rfc_fpr, rfc_tpr, _ = roc_curve(y_test, rfc_probs)

path1 = F"/content/gdrive/My Drive/Colab Notebooks/rfc_model"
rfc_weights = pickle.dump(rfc,open(path1,'wb'))

#Making the "KNN" model
neigh = KNeighborsClassifier(n_neighbors = 3)
#Fitting the model
neigh.fit(x_train, y_train)
#Predicting the output
y_pred33 = neigh.predict(x_test)
#Printing the accuracy
print(metrics.accuracy_score(y_test, y_pred33))
#Printing the precision
print(metrics.precision_score(y_test,y_pred33))
#Printing the recall
print(metrics.recall_score(y_test,y_pred33))
#Printing the Confusion Matrix
print("Confusion Matrix(in format [TN,FN],[FP,TP]):",confusion_matrix(y_test,y_pred33))

neigh_probs=neigh.predict_proba(x_test)
neigh_probs=neigh_probs[:,1]
neigh_auc=roc_auc_score(y_test,neigh_probs)
neigh_fpr, neigh_tpr, _ = roc_curve(y_test, neigh_probs)

path1 = F"/content/gdrive/My Drive/Colab Notebooks/neigh_model"
neigh_weights = pickle.dump(neigh,open(path1,'wb'))

#Making the "AdaBoost Classifier" model
adb = AdaBoostClassifier()
#Fitting the model
adb.fit(x_train, y_train)
#Predicting the output
y_pred6 = adb.predict(x_test)
#Printing the accuracy
print(metrics.accuracy_score(y_test, y_pred6))
#Printing the precision
print(metrics.precision_score(y_test,y_pred6))
#Printing the recall
print(metrics.recall_score(y_test,y_pred6))
#Printing the Confusion Matrix
print("Confusion Matrix(in format [TN,FN],[FP,TP]):",confusion_matrix(y_test,y_pred6))

adb_probs=adb.predict_proba(x_test)
adb_probs=adb_probs[:,1]
adb_auc=roc_auc_score(y_test,adb_probs)
adb_fpr, adb_tpr, _ = roc_curve(y_test, adb_probs)

path1 = F"/content/gdrive/My Drive/Colab Notebooks/adb_model"
adb_weights = pickle.dump(adb,open(path1,'wb'))

clf_sgd = SGDClassifier(loss="hinge", penalty="l2")
clf_sgd.fit(x_train, y_train)
y_pred7 = clf_sgd.predict(x_test)
#Printing the accuracy
print(metrics.accuracy_score(y_test, y_pred7))
#Printing the precision
print(metrics.precision_score(y_test,y_pred7))
#Printing the recall
print(metrics.recall_score(y_test,y_pred7))
#Printing the Confusion Matrix
print("Confusion Matrix(in format [TN,FN],[FP,TP]):",confusion_matrix(y_test,y_pred7))

path1 = F"/content/gdrive/My Drive/Colab Notebooks/sgd_model"
sgd_weights = pickle.dump(clf_sgd,open(path1,'wb'))

clf_svm = svm.SVC()
clf_svm.fit(x_train, y_train)
y_pred8 = clf_svm.predict(x_test)
#Printing the accuracy
print(metrics.accuracy_score(y_test, y_pred8))
#Printing the precision
print(metrics.precision_score(y_test,y_pred8))
#Printing the recall
print(metrics.recall_score(y_test,y_pred8))
#Printing the Confusion Matrix
print("Confusion Matrix(in format [TN,FN],[FP,TP]):",confusion_matrix(y_test,y_pred8))

path1 = F"/content/gdrive/My Drive/Colab Notebooks/svm_model"
svm_weights = pickle.dump(clf_svm,open(path1,'wb'))

"""Trying different regularization methods"""

#Ridge Regression
clf_ridge = Ridge(alpha=0.01)
clf_ridge.fit(x_train, y_train)
y_pred11 = clf_ridge.predict(x_test)
temp1 = mean_squared_error(y_test,clf_ridge.predict(x_test))
print(temp1)

path1 = F"/content/gdrive/My Drive/Colab Notebooks/ridge_regularization"
ridge_weights = pickle.dump(clf_ridge,open(path1,'wb'))

#Lasso Regression
clf_lasso = Lasso(alpha=0.01)
clf_lasso.fit(x_train, y_train)
y_pred12 = clf_lasso.predict(x_test)
temp2 = mean_squared_error(y_test,clf_lasso.predict(x_test))
print(temp2)

path1 = F"/content/gdrive/My Drive/Colab Notebooks/lasso_regression"
lasso_weights = pickle.dump(clf_lasso,open(path1,'wb'))

"""Output visualization and analysis

Heatmap after feature selection
"""

plt.figure(figsize=(16,10), dpi= 80)
df.corr()
dataplot = sns.heatmap(df.corr(), cmap="YlGnBu", annot=True)

#Actual Values and Predicted values where predicted values are different
temp_true = []
temp_pred = []
temp_index = []
temp_np_pred = np.array(y_pred3)
temp_np_test = np.array(y_test)
for i in range(len(temp_np_test)):
  if temp_np_test[i] != temp_np_pred[i]:
    temp_true.append(temp_np_test[i])
    temp_pred.append(temp_np_pred[i])
    temp_index.append(i)
plt.scatter(temp_index,temp_true)
plt.scatter(temp_index,temp_pred)
plt.title("Actual Values and Predicted values where predicted are different")
plt.ylabel("Value")
plt.xlabel("Input Number")
plt.legend(["Actual Values","Predicted Values"])
plt.show()

#Error vs number of estimators for Random Forest Classifier model
num_estimators = [i for i in range(10,1000,50)]
error_num_list = []
for num in num_estimators:
  rfc = RandomForestClassifier(n_estimators=num)
  rfc.fit(x_train, y_train)
  y_pred_new = rfc.predict(x_test)
  mse = mean_squared_error(y_test, y_pred_new)
  error_num_list.append(mse)
plt.plot(num_estimators,error_num_list)
plt.title("Error vs number of estimators")
plt.ylabel("Error")
plt.xlabel("Number of estimatores")
plt.show()

#Accuracy vs number of estimators for Random Forest Classifier model
num_estimators = [i for i in range(10,1000,50)]
acc_num_list = []
for num in num_estimators:
  rfc = RandomForestClassifier(n_estimators=num)
  rfc.fit(x_train, y_train)
  y_pred_new = rfc.predict(x_test)
  acc_temp = metrics.accuracy_score(y_test, y_pred_new)
  acc_num_list.append(acc_temp)
plt.plot(num_estimators,acc_num_list)
plt.title("Accuracy vs number of estimators")
plt.ylabel("Accuracy")
plt.xlabel("Number of estimators")
plt.show()

#Precision vs number of estimators for Random Forest Classifier model
num_estimators = [i for i in range(10,1000,50)]
acc_num_list = []
for num in num_estimators:
  rfc = RandomForestClassifier(n_estimators=num)
  rfc.fit(x_train, y_train)
  y_pred_new = rfc.predict(x_test)
  acc_temp = metrics.precision_score(y_test, y_pred_new)
  acc_num_list.append(acc_temp)
plt.plot(num_estimators,acc_num_list)
plt.title("Precision vs number of estimators")
plt.ylabel("Precision")
plt.xlabel("Number of estimators")
plt.show()

#Recall vs number of estimators for Random Forest Classifier model
num_estimators = [i for i in range(10,1000,50)]
acc_num_list = []
for num in num_estimators:
  rfc = RandomForestClassifier(n_estimators=num)
  rfc.fit(x_train, y_train)
  y_pred_new = rfc.predict(x_test)
  acc_temp = metrics.recall_score(y_test, y_pred_new)
  acc_num_list.append(acc_temp)
plt.plot(num_estimators,acc_num_list)
plt.title("recall vs number of estimators")
plt.ylabel("recall")
plt.xlabel("Number of estimators")
plt.show()

#Error vs max depth for Random Forest Classifier model
max_depth = [i for i in range(10,1000,50)]
error_num_list = []
for num in max_depth:
  rfc = RandomForestClassifier(max_depth=num)
  rfc.fit(x_train, y_train)
  y_pred_new = rfc.predict(x_test)
  mse = mean_squared_error(y_test, y_pred_new)
  error_num_list.append(mse)
plt.plot(max_depth,error_num_list)
plt.title("Error vs Max depth")
plt.ylabel("Error")
plt.xlabel("Depth")
plt.show()

#Accuacy vs max depth for Random Forest Classifier model
max_depth = [i for i in range(10,1000,50)]
acc_num_list = []
for num in max_depth:
  rfc = RandomForestClassifier(max_depth=num)
  rfc.fit(x_train, y_train)
  y_pred_new = rfc.predict(x_test)
  acc_temp = metrics.accuracy_score(y_test, y_pred_new)
  acc_num_list.append(acc_temp)
plt.plot(max_depth,acc_num_list)
plt.title("Accuracy vs Max depth")
plt.ylabel("Accuracy")
plt.xlabel("Depth")
plt.show()

"""Feature importance plot based on Random Forest Classifier model"""

features1=df.columns
importances1 = rfc.feature_importances_
indices1 = np.argsort(importances1)

plt.figure(1)
plt.title('Feature Importances')
plt.barh(range(len(indices1)), importances1[indices1], color='b', align='center')
plt.yticks(range(len(indices1)), features1[indices1])
plt.xlabel('Relative Importance')

#Accuracy vs number of neighbours for KNN model
num_neigh = [i for i in range(1,11)]
accuracy_temp_list = []
for num in num_neigh:
  neigh = KNeighborsClassifier(n_neighbors = num)
  neigh.fit(x_train, y_train)
  y_pred_new = neigh.predict(x_test)
  accuracy_temp = metrics.accuracy_score(y_test, y_pred_new)
  accuracy_temp_list.append(accuracy_temp)
plt.plot(num_neigh,accuracy_temp_list)
plt.title("accuracy vs number of neighbours")
plt.ylabel("accuracy")
plt.xlabel("Number of neighbours")
plt.show()

#Precision vs number of neighbours for KNN Model
num_neigh = [i for i in range(1,11)]
precision_temp_list = []
for num in num_neigh:
  neigh = KNeighborsClassifier(n_neighbors = num)
  neigh.fit(x_train, y_train)
  y_pred_new = neigh.predict(x_test)
  precision_temp = metrics.precision_score(y_test, y_pred_new)
  precision_temp_list.append(precision_temp)
plt.plot(num_neigh,precision_temp_list)
plt.title("precision vs number of neighbours")
plt.ylabel("precision")
plt.xlabel("Number of neighbours")
plt.show()

#Recall vs number of neighbours for KNN Model
num_neigh = [i for i in range(1,11)]
recall_temp_list = []
for num in num_neigh:
  neigh = KNeighborsClassifier(n_neighbors = num)
  neigh.fit(x_train, y_train)
  y_pred_new = neigh.predict(x_test)
  recall_temp = metrics.recall_score(y_test, y_pred_new)
  recall_temp_list.append(recall_temp)
plt.plot(num_neigh,recall_temp_list)
plt.title("recall vs number of neighbours")
plt.ylabel("recall")
plt.xlabel("Number of neighbours")
plt.show()

#Error vs number of neighbours for KNN Model
num_neigh = [i for i in range(1,11)]
error_temp_list = []
for num in num_neigh:
  neigh = KNeighborsClassifier(n_neighbors = num)
  neigh.fit(x_train, y_train)
  y_pred_new = neigh.predict(x_test)
  mse = mean_squared_error(y_test, y_pred_new)
  error_temp_list.append(mse)
plt.plot(num_neigh,error_temp_list)
plt.title("error vs number of neighbours")
plt.ylabel("error")
plt.xlabel("Number of neighbours")
plt.show()

"""Plotting learning curves"""

def learning_curves(metric):
  train_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(),df2,df3,  scoring=metric)
  train_mean = np.mean(train_scores, axis=1)
  train_std = np.std(train_scores, axis=1)
  print(test_scores )

  test_mean = np.mean(test_scores, axis=1)
  test_std = np.std(test_scores, axis=1)
  plt.subplots(1, figsize=(10,10))
  plt.plot(train_sizes, train_mean, '--', color="#111111",  label="Training score")
  plt.plot(train_sizes, test_mean, color="#111111", label="Cross-validation score")
  plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color="#DDDDDD")
  plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color="#DDDDDD")

  plt.title("Learning Curve")
  plt.xlabel("Training Set Size"), plt.ylabel(metric+" Score"), plt.legend(loc="best")
  plt.tight_layout()
  plt.show()

"""Plot for proportion of variance v/s principal component"""

PC_values = np.arange(pca.n_components_) + 1
plt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=2)
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Proportion of Variance Explained')
plt.show()

"""Plotting ROC curve"""

plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
plt.plot(dtc_fpr, dtc_tpr, marker='.', label='Decision Tree')
plt.plot(gnb_fpr, gnb_tpr, marker='.', label='Gaussian Naive Bayes')
plt.plot(rfc_fpr, rfc_tpr, marker='.', label='Random Forest Classifier')
plt.plot(adb_fpr, adb_tpr, marker='.', label='AdaBoost')
plt.plot(neigh_fpr, neigh_tpr, marker='.', label='KNN')
# plt.plot(clf_sgd_fpr, clf_sgd_tpr, marker='.', label='SGD Classifier')
# plt.plot(clf_svm_fpr, clf_svm_tpr, marker='.', label='SVM')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()

"""Plot for elbow method graph(error v/s number of clusters) for k-means clustering(Unsupervised learning)"""

plot =[]
for i in range(1, 5):
    kmeans = KMeans(n_clusters = i).fit(x_train)
    kmeans.fit(x_train)
    plot.append(kmeans.inertia_)
plt.plot(range(1, 5), plot)
plt.title('Elbow method')
plt.xlabel('No of clusters')
plt.ylabel('Error')
plt.show()

"""Density plot for the output column"""

#For "TenYearCHD" feature column
plt.figure(figsize=(6,5), dpi= 80)
sns.kdeplot(y_res, shade=True, color="deeppink", alpha=0.5)
plt.title('Density Plot of Ten Year CHD', fontsize=12)
plt.legend()
plt.show()